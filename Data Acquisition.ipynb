{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Ahib4ZvvJWmENv91ck8RsPZSUSMZ7SWG","timestamp":1674499754521}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"yQDtyMM9zpLt"},"source":["spogpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gh2ewE9TztT8"},"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VU5S6bj6T4GD"},"source":["!pip install lightkurve\n","!pip install sktime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxLb0F8CbJR_"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yun1NGQgbK8T"},"source":["book_local = 'shallue_local_curves_'\n","book_global = 'shallue_global_curves_'\n","path_input = \"/content/drive/MyDrive/Iniciação Científica/IC_Exoplanetas_2022_Experimento/Base de Dados/lighkurve_KOI_dataset.csv\"\n","path_local = '/content/drive/MyDrive/Iniciação Científica/IC_Exoplanetas_2022_Experimento/Base de Dados/Resultados teste/' + book_local + '.xlsx'\n","path_global = '/content/drive/MyDrive/Iniciação Científica/IC_Exoplanetas_2022_Experimento/Base de Dados/Resultados teste/' + book_global + '.xlsx'\n","\n","import pandas as pd\n","import numpy as np\n","import time\n","from lightkurve import search_lightcurve \n","\n","lc = pd.read_csv(path_input, sep = \",\") \n","lc = lc[['kepid','koi_disposition','koi_period','koi_time0bk','koi_duration','koi_quarters']]\n","\n","lc.shape\n","print('total inicial de curvas: %d\\n'%(lc.shape[0]))\n","\n","lc = lc.dropna()\n","lc = lc[lc.koi_disposition != 'CANDIDATE']\n","lc = lc.reset_index(drop=True)\n","print('falsos positivos: %d, confirmados: %d\\n\\ntotal atualizado: %d\\n'%((lc.koi_disposition == 'FALSE POSITIVE').sum(),(lc.koi_disposition == 'CONFIRMED').sum(),lc.shape[0]))\n","\n","perc_class = ((lc.koi_disposition == 'FALSE POSITIVE').sum()*100)/lc.shape[0]\n","print('falsos positivos: %.2f %% confirmados: %.2f %% \\n'%(perc_class,100-perc_class))\n","\n","import sys\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","\n","curvas_locais = []\n","labels_locais = []\n","curvas_globais = []\n","labels_globais = []\n","start_time = time.time()\n","for index, row in lc[5000:6000].iterrows():\n","  period, t0, duration_hours = row[2], row[3], row[4]\n","\n","  try:\n","\n","    lcs = search_lightcurve(str(row[0]), author='Kepler', cadence='long').download_all()\n","\n","    if (lcs != None):\n","\n","      lc_raw = lcs.stitch()\n","      lc_raw.flux.shape\n","\n","      lc_clean = lc_raw.remove_outliers(sigma=3)\n","\n","      temp_fold = lc_clean.fold(period, epoch_time=t0)\n","      fractional_duration = (duration_hours / 24.0) / period\n","      phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 1.5)\n","      transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n","\n","      lc_flat, trend_lc = lc_clean.flatten(return_trend=True, mask=transit_mask)\n","\n","      lc_fold = lc_flat.fold(period, epoch_time=t0)\n","\n","      #global preprocessing-----------------------------------------------------\n","      lc_global = lc_fold.bin(bins=2001).normalize() - 1\n","      lc_global = (lc_global / np.abs(np.nanmin(lc_global.flux)) ) * 2.0 + 1\n","      lc_global.flux.shape\n","      #global preprocessing-----------------------------------------------------\n","    \n","      phase_mask = (lc_fold.phase > -4*fractional_duration) & (lc_fold.phase < 4.0*fractional_duration)\n","      lc_zoom = lc_fold[phase_mask]\n","\n","      #local preprocessing------------------------------------------------------\n","      lc_local = lc_zoom.bin(bins=201).normalize() - 1\n","      lc_local = (lc_local / np.abs(np.nanmin(lc_local.flux)) ) * 2.0 + 1\n","      lc_local.flux.shape\n","      #local--------------------------------------------------------------------\n","\n","      labels_locais.append(row[1])\n","      curvas_locais.append(lc_local.flux.value)\n","\n","      labels_globais.append(row[1])\n","      curvas_globais.append(lc_global.flux.value)\n","\n","      print(index, 'OK')\n","\n","    else:\n","      print(index, 'not downloaded')  \n","    \n","  except Exception as e:\n","    print(index, e)\n","\n","t = time.time() - start_time   \n","\n","print('Tempo para importar curvas de luz: %f seconds\\n' %t)\n","\n","dataset_global = pd.DataFrame(curvas_globais)\n","dataset_local = pd.DataFrame(curvas_locais)\n","\n","for i in range(1,len(curvas_globais)):\n","    if len(curvas_globais[i]) != len(curvas_globais[i-1]):\n","        print(\"A curva %d possui tamanho diferente das demais curvas. Tamanho: %d\"%(i,len(curvas_globais[i])))\n","print(\"Caso nenhuma das curvas apresente tamanho diferente, todas as curvas GLOBAIS possuem o total de %d pontos cada.\\n\"%len(curvas_globais[0]))\n","\n","for i in range(1,len(curvas_locais)):\n","    if len(curvas_locais[i]) != len(curvas_locais[i-1]):\n","        print(\"A curva %d possui tamanho diferente das demais curvas. Tamanho: %d\"%(i,len(curvas_locais[i])))\n","print(\"Caso nenhuma das curvas apresente tamanho diferente, todas as curvas LOCAIS possuem o total de %d pontos cada.\\n\"%len(curvas_locais[0]))\n","\n","print(\"Quantidade de NaN na base GLOBAL: %s\"%dataset_global.isna().sum(axis=1).sum())\n","print(\"Quantidade de NaN na base LOCAL: %s\\n\"%dataset_local.isna().sum(axis=1).sum())\n","\n","perc_nan_glob = (dataset_global.isna().sum(axis=1).sum()*100)/dataset_global.count(axis=1).sum()\n","print(\"Porcentagem de valores do dataset GLOBAL substituídos na interpolação: %.2f %%\"%perc_nan_glob)\n","perc_nan_loc = (dataset_local.isna().sum(axis=1).sum()*100)/dataset_local.count(axis=1).sum()\n","print(\"Porcentagem de valores do dataset LOCAL substituídos na interpolação: %.2f %%\"%perc_nan_loc)\n","\n","dataset_global = dataset_global.interpolate(axis=1)\n","dataset_local = dataset_local.interpolate(axis=1)\n","\n","print(\"Quantidade de NaN na base GLOBAL após interpolação: %s --> deve sempre ser zero\"%dataset_global.isna().sum(axis=1).sum())\n","print(\"Quantidade de NaN na base LOCAL após interpolação: %s --> deve sempre ser zero\"%dataset_local.isna().sum(axis=1).sum())\n","\n","labels_glob = pd.Series(labels_globais)\n","labels_loc = pd.Series(labels_locais)\n","dataset_global['label'] = labels_glob\n","dataset_local['label'] = labels_loc\n","\n","dataset_global.to_csv(path_global,index=False)  \n","dataset_local.to_csv(path_local,index=False)"],"execution_count":null,"outputs":[]}]}